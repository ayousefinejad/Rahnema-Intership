{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer,one_hot\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# import string\n",
    "from tensorflow.keras.layers import  Embedding,Dense,Conv1D,Input,Flatten,LeakyReLU,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# اخبار"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"E:\\\\every_thing_about_python\\\\rahnamaKalej\\\\machine learning bootcamp\\\\DataSets\\\\MINDlarge_train\\\\news.tsv\"\n",
    "        ,sep='\\t',header=None,)\n",
    "\n",
    "news.columns  =[\"NewsID\"\n",
    "        ,\"Category\",\"SubCategory\",\"Title\",\"Abstract\"\n",
    "            ,\"URL\",\"Title Entities\",\"Abstract Entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# کاربران"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"E:\\\\every_thing_about_python\\\\rahnamaKalej\\\\machine learning bootcamp\\\\DataSets\\\\MINDlarge_train\\\\behaviors.tsv\"\n",
    "        ,sep='\\t',header=None,)\n",
    "\n",
    "users.columns = [\"impression ID\",\"UserID\",\"Time\",\"History\",\"Impressions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (فیلتر اول) ساخت کلاس برای لایه‌ی اول معماری"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Favorite_Categories():\n",
    "\n",
    "    def __init__(self , users ,news):\n",
    "\n",
    "        \n",
    "        self.news                   = (news.copy(deep=True)).set_index(\"NewsID\")\n",
    "\n",
    "        self.users                  = users.copy(deep=True)\n",
    "        self.number_of_categoeies   = len(set(self.news.Category))\n",
    "        \n",
    "        self.users.fillna(\" \",inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.category_of_news       = list(set(self.news.Category))\n",
    "\n",
    "\n",
    "    def get_favorite_categories_in_term(self,userid):\n",
    "\n",
    "        #  تاریخچه ی اخبار مشاهده شده توسط کاربر\n",
    "        \n",
    "        history                     = \" \".join(set(self.users[self.users.UserID==f\"U{userid}\"].History)).split()\n",
    "    \n",
    "\n",
    "        category_history            = [ self.news.loc[news].Category    for news in history]\n",
    "        category_history            = ' '.join(category_history)\n",
    "        \n",
    "        favorite_dict               ={ f\"{category}\":category_history.count(category) \n",
    "                                        for category in self.category_of_news if category in category_history }\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if favorite_dict:\n",
    "            \n",
    "            Eta                         = sum([v    for k,v in favorite_dict.items()])\n",
    "            ProbabilityDistribution     = { k:v/Eta   for k,v in favorite_dict.items()}\n",
    "\n",
    "            #  در این لیست ، دسته ی اخباری که مشاهده شده اند، به ترتیب بیشترین بازدید تا کمترین بازدید چیده می شوند\n",
    "            favorit_list            =[]\n",
    "\n",
    "            while favorite_dict:\n",
    "\n",
    "                max_seen_category   = max( favorite_dict ,key=favorite_dict.get )\n",
    "                favorite_dict.pop(max_seen_category)\n",
    "\n",
    "                favorit_list.append(max_seen_category)\n",
    "\n",
    "            return favorit_list  , ProbabilityDistribution\n",
    "        \n",
    "        else:\n",
    "            ProbabilityDistribution     = { category : (1/len(self.category_of_news))   for category in self.category_of_news}\n",
    "\n",
    "            return  self.category_of_news  , ProbabilityDistribution\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "Filter1 = Favorite_Categories(users ,news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User69 likes this categories:  lifestyle, tv, entertainment, sports\n"
     ]
    }
   ],
   "source": [
    "__userid = 69\n",
    "print(f\"User{__userid} likes this categories: \",\", \".join(Filter1.get_favorite_categories_in_term(userid=__userid)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL_RS تعریف محیط برای لایه‌ی دوم معماری "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondFilterEnvironment():\n",
    "                                    \n",
    "    def __init__(self,users , news ,max_features_for_Title=5000):\n",
    "        \n",
    "        # max_feature_news ==> در یک متن پیدا میکند Tfidf حداکثر تعداد لغاتی که بردار \n",
    "\n",
    "        # تمامی کاربران\n",
    "        self.users                  = users.copy(deep=True)\n",
    "        # خالی قرار داده میشود  String  یک  NAN برای کاربرانی که تاریخچه ای ندارند، به جای \n",
    "        self.users.History.fillna(\" \",inplace=True)\n",
    "        \n",
    "        self.news                   = (news.copy(deep=True)).set_index(\"NewsID\")\n",
    "        self.number_of_categoeies   = len(set(self.news.Category))\n",
    "\n",
    "        #  تمامی کلماتی که برای پردازش نیاز داریم \n",
    "        AllCategories               = \" \".join(set(self.news.Category))\n",
    "        AllSubCategories            = \" \".join(set(self.news.SubCategory))\n",
    "\n",
    "        #  اعداد از عناوین اخبار حذف میشوند\n",
    "        AllWordsFromTitle           = \" \".join(self.news.Title)\n",
    "        AllWordsFromTitle           = re.sub('[0-9]','',AllWordsFromTitle) \n",
    "\n",
    "\n",
    "        #    هاSubCategory   ها  و یاCategory به دلیل این که ممکن است بعضی از \n",
    "        # استفاده شد  TFIDF تشخیص داده نشود، به همین منظور از سه عدد  TFIDF توسط \n",
    "        \n",
    "        NumFeatures                 = max_features_for_Title - (len((set(self.news.Category))) + len(set(self.news.SubCategory)))\n",
    "        \n",
    "        self.NewsToArray_Category   = TfidfVectorizer(token_pattern=r\"\\S+\")\n",
    "        self.NewsToArray_SubCategory= TfidfVectorizer(token_pattern=r\"\\S+\")\n",
    "        self.NewsToArray_Title      = TfidfVectorizer(max_features=NumFeatures,stop_words=\"english\")\n",
    "\n",
    "        self.NewsToArray_Category.fit([AllCategories])\n",
    "        self.NewsToArray_SubCategory.fit([AllSubCategories])\n",
    "        self.NewsToArray_Title.fit([AllWordsFromTitle])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # DO NOT USE THIS METHOD!!!!!!!!!!!!!!!!!!!!!!!!!!!!   \n",
    "    # this is a protected method that gives state  in vector form and Text form\n",
    "    def _get_state(self,userid ):\n",
    "\n",
    "        UserHistory     = \" \".join(set(self.users[self.users.UserID==f\"U{userid}\"].History)).split()\n",
    "        \n",
    "        Category        = \"\"   \n",
    "        \n",
    "        SubCategory     = \"\"  \n",
    "\n",
    "        Title           = \"\"\n",
    "        \n",
    "        for news in UserHistory:\n",
    "\n",
    "            Category   += \" \" + self.news.loc[news].Category\n",
    "            SubCategory+= \" \" + self.news.loc[news].SubCategory\n",
    "            Title      += \" \" + self.news.loc[news].Title\n",
    "\n",
    "        # با کمک این بردار میتوان به ویژگی های متون خبری ای که کاربر قبلا مشاهده کرده دست یافت\n",
    "\n",
    "        CategoryVec       = self.NewsToArray_Category.transform([Category]).toarray()*100\n",
    "        SubCategoryVec    = self.NewsToArray_SubCategory.transform([SubCategory]).toarray()*100\n",
    "        TitleVec          = self.NewsToArray_Title.transform([Title]).toarray()*100\n",
    "        \n",
    "        StateVec          = np.hstack([np.squeeze(CategoryVec) ,np.squeeze(SubCategoryVec) ,np.squeeze(TitleVec)])[np.newaxis ,:]\n",
    "        StateText         = [Category , SubCategory , Title]     \n",
    "\n",
    "        return StateText , StateVec\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # DO NOT USE THIS METHOD!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # this is a protected method \n",
    "    def _get_state_actionReward_newState(self,userid ):\n",
    "        \n",
    "        StateText ,StateVector                    = self._get_state(userid)\n",
    "\n",
    "\n",
    "        #  تمامی اخباری که به کاربر پیشنهاد شده است.\n",
    "        Impressions                               = \" \".join(self.users[self.users.UserID==f\"U{userid}\"].Impressions).split()\n",
    "\n",
    "        #  تعریف یک دیکشنری برای این که مشخض شود هر کدام از اخبار پیشنهادی به کاربر، دیده شده است یا خیر\n",
    "        ImpressionsDict                           = { im.split(\"-\")[0]:int(im.split(\"-\")[-1])     for im in Impressions}\n",
    "        # /////////////////////\n",
    "        \n",
    "        # به روز رسانی گذشته ی کاربر\n",
    "        # NewStateText                              =  \"\"\n",
    "        # NewStateText                              += StateText\n",
    "\n",
    "\n",
    "        NewStateText_Category                     = StateText[0][::]\n",
    "        NewStateText_SubCategory                  = StateText[1][::]\n",
    "        NewStateText_Title                        = StateText[2][::]\n",
    "\n",
    "    \n",
    "\n",
    "        LikedNewsText_Category                    = \"\"\n",
    "        LikedNewsText_SubCategory                 = \"\"\n",
    "        LikedNewsText_Tile                        = \"\"\n",
    "\n",
    "\n",
    "        DisLikedNewsText_Category                 = \"\"\n",
    "        DisLikedNewsText_SubCategory              = \"\"\n",
    "        DisLikedNewsText_Tile                     = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        Reward                                    =  19\n",
    "        PunishMent                                =  -8\n",
    "\n",
    "        #  به ازای هر خبری که مشاهده شده 19 امتیاز مثبت و به ازای هر خبری که مشاهده نشده 8 امتیاز منفی دریافت میشود\n",
    "        RecievedRewards                           = {\"Punishment\":0 ,\"Reward\":0}\n",
    "\n",
    "        for news,seen in ImpressionsDict.items():\n",
    "            \n",
    "            Category                              = self.news.loc[news].Category\n",
    "            Subcategory                           = self.news.loc[news].SubCategory\n",
    "            Title                                 = self.news.loc[news].Title\n",
    "\n",
    "\n",
    "            #به روز رسانی میشود newstate اگر کاربر خبر را مشاهده کرده باشد، پس بردار مربوط به ویژگی های کاربر به در غالب \n",
    "            # اضافه میشود  LikedNews  همچنین کلمات استفاده شده در این خبر برای کشف ویژگی های خبر مورد علاقه ی کاربر به متغیر\n",
    "            if seen==1:\n",
    "\n",
    "                NewStateText_Category                 += \" \" + Category\n",
    "                NewStateText_SubCategory              += \" \" + Subcategory\n",
    "                NewStateText_Title                    += \" \" + Title\n",
    "\n",
    "\n",
    "                LikedNewsText_Category                += \" \" + Category\n",
    "                LikedNewsText_SubCategory             += \" \" + Subcategory\n",
    "                LikedNewsText_Tile                    += \" \" + Title\n",
    "\n",
    "                RecievedRewards[\"Reward\"]             += Reward\n",
    "            \n",
    "            else:\n",
    "               \n",
    "\n",
    "                DisLikedNewsText_Category             += \" \" + Category\n",
    "                DisLikedNewsText_SubCategory          += \" \" + Subcategory\n",
    "                DisLikedNewsText_Tile                 += \" \" + Title\n",
    "\n",
    "                RecievedRewards[\"Punishment\"]         += PunishMent\n",
    "\n",
    "        # گذشته ی به روز رسانی شده ی کاربر\n",
    "    \n",
    "        \n",
    "        NewStateVector_Category                    = self.NewsToArray_Category.transform([NewStateText_Category]).toarray()*100\n",
    "        NewStateVector_SubCategory                 = self.NewsToArray_SubCategory.transform([NewStateText_SubCategory]).toarray()*100\n",
    "        NewStateVector_Title                       = self.NewsToArray_Title.transform([NewStateText_Title]).toarray()*100\n",
    "\n",
    "        NewStateVector                             = np.hstack([np.squeeze(NewStateVector_Category),np.squeeze(NewStateVector_SubCategory),np.squeeze(NewStateVector_Title)])[np.newaxis ,:]\n",
    "\n",
    "        # ویژگی های متون خبری ای که کاربر دوست داشته\n",
    "        \n",
    "        LikedNewsVector_Category                    = self.NewsToArray_Category.transform([LikedNewsText_Category]).toarray()*100\n",
    "        LikedNewsVector_SubCategory                 = self.NewsToArray_SubCategory.transform([LikedNewsText_SubCategory]).toarray()*100\n",
    "        LikedNewsVector_Title                       = self.NewsToArray_Title.transform([LikedNewsText_Tile]).toarray()*100\n",
    "\n",
    "        LikedNewsVector                             = np.hstack([np.squeeze(LikedNewsVector_Category),np.squeeze(LikedNewsVector_SubCategory),np.squeeze(LikedNewsVector_Title)])[np.newaxis ,:]\n",
    "       \n",
    "        # ویژگی های متون خبری ای که کاربر دوست نداشته\n",
    "        \n",
    "        DisLikedNewsVector_Category                = self.NewsToArray_Category.transform([DisLikedNewsText_Category]).toarray()*100\n",
    "        DIsLikedNewsVector_SubCategory             = self.NewsToArray_SubCategory.transform([DisLikedNewsText_SubCategory]).toarray()*100\n",
    "        DisLikedNewsVector_Title                   = self.NewsToArray_Title.transform([DisLikedNewsText_Tile]).toarray()*100\n",
    "\n",
    "        DisLikedNewsVector                         = np.hstack([np.squeeze(DisLikedNewsVector_Category),np.squeeze(DIsLikedNewsVector_SubCategory),np.squeeze(DisLikedNewsVector_Title)])[np.newaxis ,:]\n",
    "\n",
    "\n",
    "        Result                                     ={ \"state\":StateVector , \"LikedNewsVector\":LikedNewsVector ,\n",
    "                                                      \"DisLikedNewsVector\":DisLikedNewsVector ,\"newstate\":NewStateVector  }\n",
    "        \n",
    "        \n",
    "        return {**Result , **RecievedRewards}\n",
    "    \n",
    "\n",
    "    #  ONLY USE THIS METHOD*****************************\n",
    "    def get_transitions(self,userid:int)->(dict):\n",
    "\n",
    "        Transitions =  self._get_state_actionReward_newState(userid)\n",
    "\n",
    "        return Transitions\n",
    "\n",
    "\n",
    "\n",
    "env = SecondFilterEnvironment(users , news ,5000)\n",
    "\n",
    "\n",
    "\n",
    "# del users , news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 5000), (1, 5000), (1, 5000), (1, 5000))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_transitions(83)[\"LikedNewsVector\"].shape , env.get_transitions(212)[\"DisLikedNewsVector\"].shape ,env.get_transitions(212)[\"state\"].shape,env.get_transitions(212)[\"newstate\"].shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   ساخت مدل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " User Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " News Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 1, 128)       640128      ['User Features[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 1, 128)       640128      ['News Features[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 1, 128)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 1, 128)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['leaky_re_lu_1[0][0]',          \n",
      "                                                                  'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          65792       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            514         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,379,586\n",
      "Trainable params: 1,379,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " User Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " News Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 1, 128)       640128      ['User Features[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 1, 128)       640128      ['News Features[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 1, 128)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 1, 128)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 128)          0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 128)          0           ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          16512       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          16512       ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 128)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 256)          0           ['leaky_re_lu_6[0][0]',          \n",
      "                                                                  'leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          65792       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 256)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            514         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,379,586\n",
      "Trainable params: 1,379,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(StateShapeUser,StateShapeNews):\n",
    "    \n",
    "\n",
    "    UserFeatures = Input(shape=StateShapeUser,name=\"User Features\")\n",
    "\n",
    "    NewsFeatures = Input(shape=StateShapeNews,name=\"News Features\")\n",
    "\n",
    "    user         = Conv1D(128,1,input_shape=(StateShapeUser))(UserFeatures)\n",
    "    user         = LeakyReLU()(user)\n",
    "    user         = Flatten()(user)\n",
    "    user         = Dense(128)(user)\n",
    "    user         = LeakyReLU()(user)\n",
    "\n",
    "\n",
    "    news         = Conv1D(128,1,input_shape=(StateShapeNews))(NewsFeatures)\n",
    "    news         = LeakyReLU()(news)\n",
    "    news         = Flatten()(news)\n",
    "    news         = Dense(128)(news)\n",
    "    news         = LeakyReLU()(news)\n",
    "\n",
    "\n",
    "    concate      = Concatenate()([user ,news])\n",
    "\n",
    "    concate      = Dense(256)(concate)\n",
    "    concate      = LeakyReLU()(concate)\n",
    "\n",
    "    out          = Dense(2)(concate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model        = Model(inputs=[UserFeatures,NewsFeatures],outputs=out) \n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "UserStateShape  =(1,5000)\n",
    "NewsStateShape  =(1,5000)\n",
    "\n",
    "model           = create_model( UserStateShape , NewsStateShape )\n",
    "target_model    = create_model( UserStateShape , NewsStateShape )\n",
    "\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "#//////////////  loss_function( y_true , y_predict )\n",
    "loss_function = tf.keras.losses.Huber()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ساخت یک کلاس برای آموزش راحت تر مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dqn_model():\n",
    "\n",
    "    def __init__(self,env,model ,target_model ,loss_function ,optimizer ,discount_factor ,action_size= 2):\n",
    "\n",
    "        self.env            = env\n",
    "        self.action_size    = action_size\n",
    "        self.model          = model\n",
    "        self.target_model   = target_model\n",
    "        self.loss_function  = loss_function\n",
    "        self.optimizer      = optimizer\n",
    "        self.discount       = discount_factor\n",
    "        \n",
    "    # private method\n",
    "    #   this method is the core to learn policy\n",
    "    @tf.function\n",
    "    def __train(self,StateUser,StateNews ,action ,reward ,NewStateUser):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            Q1 = self.model([StateUser ,StateNews] ,training=True)\n",
    "            Q2 = Q1[::]\n",
    "\n",
    "            reward = tf.cast(reward ,tf.float32)\n",
    "            Y = reward + self.discount * tf.reduce_max( self.target_model([NewStateUser,StateNews] , training=True) ,axis=-1 )\n",
    "            \n",
    "            \n",
    "            action_index = tf.one_hot([action] , self.action_size)\n",
    "\n",
    "            Q2 = tf.tensor_scatter_nd_update(Q2 , tf.where(action_index)  , Y)\n",
    "            \n",
    "            loss = self.loss_function(Q2 ,Q1)\n",
    "\n",
    "        grad = tape.gradient(loss , self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip( grad , self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # private method\n",
    "    #  This method helps to put the above method in a for loop\n",
    "    def __learn(self,StateUser,StateNews ,action ,reward ,NewStateUser ):\n",
    "        \n",
    "        loss = self.__train(StateUser,StateNews ,action ,reward ,NewStateUser)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    # به روز رسانی شبکه هدف\n",
    "    @tf.function\n",
    "    def soft_Update(self,  target_model_weights    ,  model_weights   ,  tau=0.01):\n",
    "\n",
    "        for TM , M in zip(target_model_weights ,model_weights):\n",
    "            TM.assign(  (1-tau)*TM  +  tau * M   )\n",
    "        \n",
    "\n",
    "    #  این تابع ، با استفاده از  محیط، به شبکه ی عصبی، ویژگی های اخبار مورد علاقه ی و  کاربر مورد نظر را آموزش می دهد\n",
    "    def Learn_favorite_category(self,userid , n_epocks=1 ,early_stopping = 0.01):\n",
    "\n",
    "        transition          = self.env.get_transitions(userid) \n",
    "\n",
    "        StateUser           = transition[\"state\"][np.newaxis,:,:]\n",
    "        NewStateUser        = transition[\"newstate\"][np.newaxis,:,:]\n",
    "        LikedNewsVector     = transition[\"LikedNewsVector\"][np.newaxis,:,:]\n",
    "        DisLikedNewsVector  = transition[\"DisLikedNewsVector\"][np.newaxis,:,:]\n",
    "        ActionForLike       = 1\n",
    "        ActionForDislike    = 0\n",
    "\n",
    "        Reward              = transition[\"Reward\"]\n",
    "        Punishment          = transition[\"Punishment\"]\n",
    "\n",
    "        TotalLoss=[]\n",
    "        for epock in range(n_epocks):\n",
    "            \n",
    "            loss1          = self.__learn(StateUser,LikedNewsVector    ,ActionForLike    ,Reward     ,NewStateUser )\n",
    "            loss2          = self.__learn(StateUser,DisLikedNewsVector ,ActionForDislike ,Punishment ,NewStateUser ,)\n",
    "            \n",
    "            TotalLoss.append(loss1+loss2)\n",
    "            \n",
    "            if (loss1 + loss2)<=early_stopping:\n",
    "                break        \n",
    "                \n",
    "        MeanTotalLoss     = np.mean(TotalLoss)\n",
    "\n",
    "        return MeanTotalLoss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dqn                     = Dqn_model(env ,model ,target_model ,loss_function ,optimizer,0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  debugging for Env and DQN classes......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dqn is learning User592649\n",
      "dqn is learning User454964\n",
      "dqn is learning User278140\n",
      "dqn is learning User48575\n",
      "dqn is learning User340821\n",
      "dqn is learning User257367\n",
      "dqn is learning User703341\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.randint(low =0 ,high = 711221,size= 10):\n",
    "    \n",
    "    #می‌باشد  __train   در بالای تابع   @tf.function    مشاهده شد، مربوط به    warning  اگر در این جا \n",
    "    print(f\"dqn is learning User{i}\")\n",
    "    dqn.Learn_favorite_category(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 in progress.....\n",
      "episode 1 finished. loss = 28.54543685913086 \n",
      " ==================================================\n",
      "Episode 2 in progress.....\n",
      "episode 2 finished. loss = 31.748987197875977 \n",
      " ==================================================\n",
      "Episode 3 in progress.....\n",
      "episode 3 finished. loss = 17.15983009338379 \n",
      " ==================================================\n",
      "Episode 4 in progress.....\n",
      "episode 4 finished. loss = 33.222049713134766 \n",
      " ==================================================\n",
      "Episode 5 in progress.....\n",
      "episode 5 finished. loss = 11.866804122924805 \n",
      " ==================================================\n",
      "Episode 6 in progress.....\n",
      "episode 6 finished. loss = 30.54372215270996 \n",
      " ==================================================\n",
      "Episode 7 in progress.....\n",
      "episode 7 finished. loss = 49.46721649169922 \n",
      " ==================================================\n",
      "Episode 8 in progress.....\n",
      "episode 8 finished. loss = 12.67637825012207 \n",
      " ==================================================\n",
      "Episode 9 in progress.....\n",
      "episode 9 finished. loss = 112.05821990966797 \n",
      " ==================================================\n",
      "Episode 10 in progress.....\n",
      "episode 10 finished. loss = 191.76950073242188 \n",
      " ==================================================\n"
     ]
    }
   ],
   "source": [
    "n_episodes              = 10   #     تعداد کل مراحل آموزشی \n",
    "length_of_episode       = 10   #    در هر بار آموزش شبکه عصبی، چند کاربر به شبکه عصبی آموزش داده میشود\n",
    "\n",
    "n_iterations            = 10   #     علایق هر کاربر، چند مرتبه به شبکه عصبی آموزش داده می‌شود\n",
    "\n",
    "number_of_users         = 711221     #   تعداد کل  کاربران موجود در دیتاست\n",
    "\n",
    "\n",
    "np.random.choice(range(number_of_users), length_of_episode)\n",
    "# range(start_index , end_index )\n",
    "total_loss              = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "   print(f\"Episode {episode+1} in progress.....\")\n",
    "\n",
    "   #یاد گرفته شود    end_index    تا کاربر شماره ی     start_index      از کاربر شماره ی    \n",
    "   # start_index          = np.random.randint( low=0   ,  high= number_of_users -length_of_episode)\n",
    "   # end_index            = start_index + length_of_episode + 1\n",
    "   \n",
    "   episode_loss         = []\n",
    "   for userid in np.random.choice(range(number_of_users), length_of_episode):\n",
    "      \n",
    "      loss_ = dqn.Learn_favorite_category(userid , n_iterations )\n",
    "\n",
    "      episode_loss.append(loss_)\n",
    "\n",
    "   episode_loss         = np.mean(episode_loss)\n",
    "   total_loss.append(episode_loss)\n",
    "\n",
    "      \n",
    "   if episode%10        ==0:\n",
    "      \n",
    "      dqn.soft_Update(dqn.target_model.trainable_variables,dqn.model.trainable_variables )\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "   print(f\"episode {episode +1} finished. loss = {episode_loss} \\n\",\"=\"*50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ساخت کلاس سیستم توصیه گر"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(SecondFilterEnvironment):\n",
    "\n",
    "    def __init__(self , Filter1:Favorite_Categories , Filter2:Model ,Users:pd.DataFrame, News:pd.DataFrame  )->None:\n",
    "\n",
    "        super().__init__(Users , News)\n",
    "        self.Filter1                    = Filter1\n",
    "        self.Filter2                    = Filter2\n",
    "\n",
    "        self.News                       =  News\n",
    "\n",
    "\n",
    "    # this is a private method\n",
    "    # Do not use this method outside of class!!!!!!!!!!!\n",
    "    def __GetNewsFeatures(self, NewsID)->np.array:\n",
    "\n",
    "        Category_Text                   =  self.news.loc[NewsID].Category\n",
    "        SubCategory_Text                =  self.news.loc[NewsID].SubCategory\n",
    "        Title_Text                      =  self.news.loc[NewsID].Title\n",
    "\n",
    "        Category_Vec                    =  self.NewsToArray_Category.transform([Category_Text]).toarray()\n",
    "        SubCategory_Vec                 =  self.NewsToArray_SubCategory.transform([SubCategory_Text]).toarray()\n",
    "        Title_Vec                       =  self.NewsToArray_Title.transform([Title_Text]).toarray()\n",
    "\n",
    "        NewsFeatures                    =  np.hstack([np.squeeze(Category_Vec),np.squeeze(SubCategory_Vec),np.squeeze(Title_Vec)])[np.newaxis,:]\n",
    "\n",
    "        return NewsFeatures\n",
    "\n",
    "    # this is a private method\n",
    "    # Do not use this method outside of class!!!!!!!!!!!\n",
    "    def __GetSeenedNews(self , UserID:int) ->set:\n",
    "        \n",
    "\n",
    "        Set_history                     = set(\" \".join(set(env.users[env.users.UserID==f\"U{UserID}\"].History)).split())\n",
    "\n",
    "        Set_Impressions                 = set([news.split(\"-\")[0]   for news  in \" \".join(env.users[env.users[\"UserID\"]==f\"U{UserID}\"][\"Impressions\"]).split()])\n",
    "        \n",
    "        Union                           = Set_history   |  Set_Impressions\n",
    "        \n",
    "        return Union\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #   Main method\n",
    "    def OfferNews(self , UserID , MaxNumberOfNews=10) ->list:\n",
    "\n",
    "        FavoriteCategories              = self.Filter1.get_favorite_categories_in_term(UserID)[-1]\n",
    "\n",
    "        sortedDict_FavoriteCategories   =  sorted(FavoriteCategories.items() , key = lambda X :X[-1] , reverse=True)\n",
    "\n",
    "        List_FavoriteCategories         = list(map(lambda X :X[ 0] ,sortedDict_FavoriteCategories))\n",
    "        List_Distribution               = list(map(lambda X :X[-1] ,sortedDict_FavoriteCategories))\n",
    "\n",
    "\n",
    "        UserFeatures                    = self._get_state(UserID)[-1]\n",
    "\n",
    "        #   با توجه به آمار کتگوری‌های دیده شده، یک لیست ایجاد میشود تا از دسته‌های محبوب، اخبار بیشتری پیشنهاد گردد\n",
    "        SampleCategories                = [np.random.choice(List_FavoriteCategories, p=List_Distribution)    for _ in range(MaxNumberOfNews)   ] \n",
    "\n",
    "        #  اخباری که قرار است پیشنهاد شوند\n",
    "        OfferedNews                     = []\n",
    "\n",
    "\n",
    "        AlreadySeenedNews               = self.__GetSeenedNews(UserID)\n",
    "\n",
    "\n",
    "        while len(OfferedNews)< MaxNumberOfNews:\n",
    "            \n",
    "        \n",
    "            SelectedCategory           =   SampleCategories.pop(0)\n",
    "\n",
    "            while True:\n",
    "                \n",
    "                NewsID                 = ((self.News[self.News[\"Category\"]==SelectedCategory]).sample(1))[\"NewsID\"].to_list()[0]\n",
    "\n",
    "                NewsFeatures           =  self.__GetNewsFeatures(NewsID)  \n",
    "\n",
    "                #   اگر آن خبر مورد نظر، خبری بود که کاربر مشاهده نکرده است\n",
    "                if NewsID not in (set(OfferedNews)  |  AlreadySeenedNews):\n",
    "\n",
    "                    if (np.argmax(model([UserFeatures[np.newaxis,:] , NewsFeatures[np.newaxis,:]  ]))) == 1:\n",
    "\n",
    "                        OfferedNews.append(NewsID)\n",
    "                        \n",
    "                        break\n",
    "            \n",
    "\n",
    "          \n",
    "                \n",
    "\n",
    "        return OfferedNews \n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "recommender = RecommenderSystem(Filter1 , model ,users,news )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# نمونه خروجی سیستم توصیه گر پس از اتصال تمامی فیلتر‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended news to User 15: {N56588,  N21346,  N6035,  N5438,  N89302,  N47810,  N6982,  N75484,  N105411,  N110420}\n"
     ]
    }
   ],
   "source": [
    "userid__    = 15\n",
    "\n",
    "R_News      =recommender.OfferNews(UserID=userid__,MaxNumberOfNews=10)\n",
    "\n",
    "\n",
    "print(f\"recommended news to User {userid__}: \" + \"{\" + \",  \".join(R_News) + \"}\" )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703cc23dfc81a25e43b5bf51938915990284a5a268e2fc63c8ffc4a4768c8d82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
